# Deep One-Class Classification

## どんなもの?
カーネルベースの一クラス分類と最小ボリューム推定からインスピレーションを受け、新しい深いADを提案する。その手法はDeep Support Vector Data Description(以下Deep SVDD)と名付ける。これはデータのネットワーク表現を包みこむ超球のボリュームを最小化するニューラルネットワークを学習する。下の図(論文中の図1)で言えば、学習中にニューラルネットワーク(∅(・;W))を介して、入力(X)には正常なデータ(黒点)のみが与えられ、出力(F)ではcを中心とする半径Rの超球が内部に点を含むように形成される。この時、ニューラルネットワークも学習する。テスト時に異常なデータ(白点)が入力されても出力された時、その点は外側に位置するようになる。  
![論文中の図1](img/DOC/fig_1.png)

(超球面の体積を最小限にすると、ネットワークはデータ点を球の中心に密接にマッピングする必要があるため、ネットワークに変動の共通因子を抽出させる必要があります。)


## 先行研究と比べてどこがすごいの?
深層学習は革新的でありAnomaly Deteciotn(以下AD)でも深いADとして有望な結果を出しているが、ADベースの目的関数を最適化することによって学習されておらず、基本的に再構築誤差(入力と出力の誤差)に依存している。  
AutoEnocoder(以下AE)等の再構築誤差を使う学習を行うものは、学習時の入力に正常なデータだけを使い、その正常なデータに含まれる多様性の一般的要因を抽出できるようにする。また、再構築誤差を異常スコアとして使い異常検知も可能である。テスト時にこの要因を含む入力は入力に近似したものを出力できるが、そうではない入力は要因を抽出できなくなるので入力とは別のものが出力される。こうして再構築誤差を使ったものは異常検知ができる。

ただし、AEはADのために作られたものではない。そのため、AEをADに適応する際に入力情報をどこまで次元圧縮するか(以下compactness)の調節が必要となる。compactnessはハイパーパラメータであり、適切なバランスを取るのは難しい。

Deep SVDDはADの使用を前提としており、データを囲む超球のボリュームを最小限に抑えることで、適切なcompactnessを得ている。

AnoGANもAEと同様でどうやってcompactnessのためにジェネレータを正規化するかが問題になる。(????)

## 技術や手法のキモはどこ?
- **各定義**  
入力X⊆Rd、出力F⊆Rpとして、ニューラルネットワークを∅(・;W):X→F、隠れ層L∈Nで重みW=(W1,...,WL)、ここでWlはl∈{1,...,L}とする。また、∅(x,W)∈Fはx∈Xの特徴表現である。

- **最初の仮定**  
Deep SVDDはWの調節とFを含む超球の最小化を同時に学習する。このとき、超級は半径R>0、中央c∈Fとする。

- **学習**  
学習データにDn={x1,...,xn}が与えられたとすると、soft-boundary(超球の境界線決定?) Deep SVDDの目標は以下の定義式(論文中の式(3))となる。  
![論文中の式3](img/DOC/fig_0.png)  
    - 第一項はカーネルSVDDと同じようにR**2を最小にすることで超球を最小化することになる。
    - 第二項は罰則項であり、ネットワークを通過したあと球の外にある点(つまり、中心からの距離が半径Rよりはるかに遠い点がある場合)のためである。ハイパーパラメーターv∈(0,1]は球のボリュームと限度違反のトレードオフを取り持つ。つまり、どれくらい球の外側の点を許容するか決められる。


## どうやって有効だと検証した?

## 議論はある?

## 次に読むべき論文は?

### 論文関連リンク

### 参考リンク

### 会議
ICML2018

### 著者/所属機関
Lukas Ruff, Robert A. Vandermeulen, Nico G ̈ornitz, Lucas Deecke, Shoaib A. Siddiqui, Alexander Binder, Emmanuel M ̈uller, Marius Kloft

### 投稿日付(yyyy/MM/dd)
2018/11/29

## コメント
